{
 "metadata": {
  "name": "",
  "signature": "sha256:3f20bf3c5345e464947ebf3e9424f487c65ad0e1fb8e86cb343fa0f93d113f22"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Praktyczny Machine Learning w Pythonie\n",
      "<br>\n",
      "<img src=\"figures/dilbert-2213.gif\">\n",
      "\n",
      "## Rozpoznawanie cyfr\n",
      "\n",
      "Nasz model b\u0119dzie sk\u0142ada\u0142 si\u0119 z 2 krok\u00f3w:\n",
      "1. Zmniejszenie ilo\u015bci wymiar\u00f3w\n",
      "2. Klasyfikacja modelem liniowym\n",
      "\n",
      "Najpierw jednak musimy zapozna\u0107 si\u0119 ze zbiorem danych."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_mldata\n",
      "mnist = fetch_mldata('MNIST original')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MNIST\n",
      "\n",
      "<img src=\"figures/mnist_originals.png\">\n",
      "\n",
      "MNIST to baza danych odr\u0119cznie napisanych cyfr oko\u0142o 20 lat temu. Ludzie s\u0105 w stanie rozpozna\u0107 ok. 99,5% cyfr z tego zbioru poprawnie.\n",
      "Zobaczymy ile nam si\u0119 uda!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "img = mnist.data[0]\n",
      "print \"Pierwsz obrazek z \", mnist.data.shape[0], \":\", img # Pixele"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mo\u017cemy sobie narysowa\u0107 wcze\u015bniej wypisan\u0105 cyfr\u0119\n",
      "import matplotlib.pylab as plt\n",
      "%matplotlib inline\n",
      "plt.imshow(img.reshape(28,28), cmap=\"gray\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "from sklearn import linear_model, decomposition\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import preprocessing\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Krok 1: wczytanie i podzielenie danych\n",
      "\n",
      "Skalowanie jest bardzo wa\u017cne. Dzi\u0119ki temu mamy \u015bredni\u0105 danej cechy 0 i wariancj\u0119 1.\n",
      "\n",
      "<img src=\"figures/prepro2.jpeg\" width=600>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Wczytujemy dane i skalujemy\n",
      "X, Y = mnist.data.astype(\"float64\"), mnist.target \n",
      "X = preprocessing.scale(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tak jak wspomnia\u0142em algorytm trenujemy na innych danych ni\u017c testujemy. **To bardzo wa\u017cne**. Do ka\u017cdych danych da si\u0119 dopasowa\u0107 taki model, kt\u00f3ry idealnie na nich odpowiada (np. poprzez zapami\u0119tanie wszystkich przyk\u0142ad\u00f3w). Je\u015bli tak zrobimy, to m\u00f3wimy \u017ce nasz model **zoverfitowa\u0142**.\n",
      "\n",
      "<img src=\"figures/underfitting-overfitting.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Dzielimy na dane trenuj\u0105ce i testuj\u0105ce\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Krok 2: dopasujmy pare modeli - uwaga! To juz trwa chwile, 70000 przykladow o 784 cechach kazdy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Na wszystkich przykladach osiaga 92% dokladnosci. \n",
      "N = 500 # Podzbi\u00f3r danych\n",
      "model = LogisticRegression() # Model z domy\u015blnymi parametrami\n",
      "model.fit(X_train[0:N], Y_train[0:N])\n",
      "\n",
      "Y_test_predicted = model.predict(X_test)\n",
      "print \"Dok\u0142adno\u015b\u0107 modelu wytrenowanego na \",N, \" przykladach to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Przyklad zaklasyfikowany jako \", model.predict(X_test[0])\n",
      "plt.imshow(X_test[0].reshape(28,28), cmap=\"gray\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Krok 3: Tworzymy caly model zmniejszaj\u0105c ilo\u015b\u0107 przyk\u0142ad\u00f3w\n",
      "\n",
      "W scikit-learn mo\u017cemy po\u0142\u0105czy\u0107 pare modeli w **pipeline**, kt\u00f3ry sam implementuje interfejs **Estimator**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = decomposition.PCA(n_components=20)\n",
      "pca.fit(X_train)\n",
      "X_train_transf = pca.transform(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "logistic = linear_model.LogisticRegression(C=0.1)\n",
      "logistic.fit(X_train_transf, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pipeline pozwala spina\u0107 obiekty typu `Estimator`. Za pomoc\u0105 pipeline mo\u017cna tworzy\u0107 bardzo skomplikowane modele, kt\u00f3re maj\u0105 bardzo prosty interfejs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Zobaczmy na dok\u0142adno\u015b\u0107 modelu\n",
      "Y_test_predicted = pipe.predict(X_test)\n",
      "print \"Dok\u0142adno\u015b\u0107 modelu wytrenowanego to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Poszukajmy najlepszych hiperparametr\u00f3w"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "N = 500\n",
      "tuned_parameters = [{'pca__n_components':[10,20,30], 'logistic__C':  [1, 10, 100, 1000]}]\n",
      "clf_fitted = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
      "clf = GridSearchCV(clf_fitted, tuned_parameters, cv=5, scoring='accuracy', verbose=4)\n",
      "clf.fit(X_train[0:N], Y_train[0:N])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Zobaczmy na dok\u0142adno\u015b\u0107 modelu\n",
      "Y_test_predicted = clf.best_estimator_.predict(X_test)\n",
      "print \"Dok\u0142adno\u015b\u0107 modelu wytrenowanego to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Poprawa o 6%! Nice**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Konkurs\n",
      "\n",
      "Wczyta\u0107 i analogicznie jak poprzednio zbudowa\u0107 najlepszy model do zbioru danych faces z scikit-learn\n",
      "\n",
      "<img src=\"figures/faces.png\">\n",
      "\n",
      "Podpowiedzi:\n",
      "\n",
      "0. Warto pami\u0119ta\u0107 o skalowaniu danych\n",
      "1. Jest to stosunkowo ma\u0142y zbi\u00f3r - prosty model nie b\u0119dzie potrzebowa\u0142 PCA\n",
      "2. Znale\u017a\u0107 najlepsze hiperparametry (tylko 1 w LogisticRegression)\n",
      "3. Spr\u00f3bowa\u0107 innego modelu (traktuj\u0105c go jako czarne pude\u0142ko z funkcja fit i predict :) ).\n",
      "    * np. SVC z kernel=\"rbf\" (ma 2 hiperparametry C, gamma)\n",
      "4. Je\u015bli dobieramy 2 hiperparametry warto u\u017cyc GridSearch (samo szuka najlepszy zestaw parametr\u00f3w) np. http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV \n",
      "5. Je\u015bli u\u017cywamy bardziej skomplikowanego modelu (np. wspomnianego SVC) to nale\u017cy u\u017cy\u0107 PCA (**lub innej techniki redukcji wymiar\u00f3w**, PCA jest bardzo prostym modelem, zobaczy\u0107 `na sklearn.demposition`)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}