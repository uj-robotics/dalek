{
 "metadata": {
  "name": "",
  "signature": "sha256:5f32cd5f78d746f3a4a763edf01704392baaeace5393078fbb479cb8bcd1258b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Praktyczny Machine Learning w Pythonie\n",
      "<br>\n",
      "<img src=\"figures/pie.gif\">\n",
      "\n",
      "## Podstawy machine learningu i scikit-learn\n",
      "\n",
      "Na tych warsztatach skupimy si\u0119 na modelach liniowych. Modele liniowe odzielaj\u0105 obiekty r\u00f3\u017cnych klas za pomoc\u0105 linii prostej. Poni\u017cszy rysunek powinien to wyja\u015bni\u0107:\n",
      "\n",
      "<img src=\"figures/twofeature_b.png\">\n",
      "\n",
      "K\u00f3\u0142kami s\u0105 r\u00f3\u017cne obiekty (konkretne irysy, odr\u0119cznie napisany cyfry, itp.)\n",
      "Na osi X i Y mamy cechy obiektu (np. d\u0142ugo\u015b\u0107 i szeroko\u015b\u0107 p\u0142atk\u00f3w), a linia odziela przyk\u0142ady r\u00f3\u017cnych klas. Chc\u0105c sklasyfikowa\u0107 **nowy** przyk\u0142ad patrzymy po kt\u00f3rej znajduje si\u0119 stronie linii. Proste! A podstawa ogromnej cz\u0119\u015bci machine learningu. Jak mamy dobre cechy, to cz\u0119sto wystarczy prosta kreska. Dlatego zwykle wi\u0119kszo\u015b\u0107 czasu w machine learningu nie jest po\u015bwi\u0119cane na znajdowanie najlepszego modelu, a zbieranie i czyszczenie danych.\n",
      "\n",
      "## 1. Scikit-learn\n",
      "\n",
      "Scikit-learn to bardzo popularny pakiet udost\u0119pniaj\u0105cy r\u00f3\u017cne algorytmy uczenia maszynowego. U\u017cywany jest w wielu firmach (w tym Microsoft, Evernote, ...). Skupimy si\u0119 na modelach liniowych. Wi\u0119kszo\u015b\u0107 obiekt\u00f3w w scikit-learn implementuje interfejs **Estimator**. Wed\u0142ug tej terminologii \"dopasowywujemy\" si\u0119 do dancyh, aby potem \"przewidzie\u0107\" klas\u0119 obiektu.\n",
      "\n",
      "```{python}\n",
      "class Estimator(object):\n",
      "  \n",
      "    def fit(self, X, y=None):\n",
      "        \"\"\"Fits estimator to data. \"\"\"\n",
      "        # set state of ``self``\n",
      "        return self\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict response of ``X``. \"\"\"\n",
      "        # compute predictions ``pred``\n",
      "        return pred\n",
      "```\n",
      "\n",
      "**Ka\u017cdy model to Estimator**, a jest ich bardzo du\u017co. Dodatkowo (do czego dojdziemy) r\u00f3\u017cne preprocessingi na danych (np skalowanie) te\u017c implementuj\u0105 ten interfejs co bardzo upraszcza prac\u0119 z pakietem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tworzymy nasz model liniowy (prost\u0105 kresk\u0119) odr\u00f3\u017cniaj\u0105cy gatunek irys\u00f3w Setosa od innych!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standardowe importowanie pakiet\u00f3w\n",
      "import numpy as np\n",
      "import matplotlib.pylab as plt\n",
      "%matplotlib inline\n",
      "from sklearn.datasets import load_iris"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### a) \u0141adujemy dane. Zapis iris.data[:,0:2] oznacza wybierz 2 pierwsze kolumny"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Za\u0142adujmy jeszcze raz dane iris\n",
      "iris = load_iris()\n",
      "X,Y = iris.data[:,0:2], iris.target\n",
      "Y = Y==0 # Troche magii \u017ceby przewidywa\u0142 tylko Iris Setosa albo \"nie Iris Setosa\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris.target_names # Setosa jest pierwsza (indeks 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array(['setosa', 'versicolor', 'virginica'], \n",
        "      dtype='|S10')"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Ok! Dopasowywujemy model. \n",
      "from sklearn.svm import SVC\n",
      "# Regresja liniowa. Jeden z wielu modeli, kt\u00f3rych ko\u0144cowym wynikiem jest linia prosta :)\n",
      "pierwszy_model = SVC() \n",
      "pierwszy_model.fit(X, Y) # Wspominany interfejs! Po zrobieniu fit mamy lini\u0119 prost\u0105"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print pierwszy_model.coef_[0] # Spr\u00f3bujcie sami \"stabowa\u0107\" co ma model w \u015brodku"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "coef_ is only available when using a linear kernel",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-60df5f1517c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mpierwszy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Spr\u00f3bujcie sami \"stabowa\u0107\" co ma model w \u015brodku\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mcoef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             raise ValueError('coef_ is only available when using a '\n\u001b[0m\u001b[0;32m    412\u001b[0m                              'linear kernel')\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: coef_ is only available when using a linear kernel"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### b) Model machine learningowy (*pierwszy_model* w naszym kodzie) mo\u017cna stre\u015bci\u0107 jako:\n",
      "\n",
      "\"Je\u015bli -2.41366749 \\* **d\u0142ugo\u015b\u0107_p\u0142atka** + 4.12584958 \\* **szeroko\u015b\u0107_p\u0142atka** >= 0 to jest to kwiatek gatunku *Iris Setosa*\"\n",
      "\n",
      "Nie uwierzycie ile modeli jest tak prostych :)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Mo\u017cna zapyta\u0107 (predict w Estimator)\n",
      "pierwszy_model.predict([-0.109, 0.451]) # Kwiatek gatunku Iris Setosa! "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### c) Wizualizacja nauczonego modelu"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tworzymy siatk\u0119 punkt\u00f3w w odst\u0119pie 1e-2\n",
      "h = 1e-2\n",
      "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                     np.arange(y_min, y_max, h))\n",
      "\n",
      "# Przewidujemy\n",
      "Z = pierwszy_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "Z = Z.reshape(xx.shape)\n",
      "\n",
      "# Rysujemy \u0142adny wykres \n",
      "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "plt.xlabel('Sepal length')\n",
      "plt.ylabel('Sepal width')\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())\n",
      "plt.xticks(())\n",
      "plt.yticks(())\n",
      "plt.title(\"pierwszy_model\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Podstawy machine learningu"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podsumujmy co wiemy:\n",
      "    \n",
      "* Machine Learning zajmuje si\u0119 tworzeniem modeli, kt\u00f3re uczymy na podstawie danych\n",
      "* Scikit-learn udost\u0119pnia wiele modeli, uczymy je za pomoc\u0105 funkcji **fit**, przewidujemy za pomoc\u0105 funkcji **predict**\n",
      "* Wa\u017cn\u0105 klas\u0105 modeli s\u0105 modele liniowe na kt\u00f3rych si\u0119 skupimy\n",
      "* Znamy podstawy numpy i matplotlib\n",
      "\n",
      "Wprowadzimy jeszcze 3 poj\u0119cia:\n",
      "* Testowanie modelu\n",
      "* Preprocessing danych\n",
      "* Hiperparametry\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### a) Testowanie modelu\n",
      "\n",
      "Chcemy wiedzie\u0107 jak dobry jest model kt\u00f3ry mamy. W tym celu zbi\u00f3r danych dzielimy na ** zbi\u00f3r trenuj\u0105cy ** i ** zbi\u00f3r testuj\u0105cy **. Uczymy si\u0119 na zbiorze trenuj\u0105cym, testujemy na testuj\u0105cym. Zwykle podsumowywujemy wynik modelu jedn\u0105 liczb\u0105, np. **dok\u0142adno\u015b\u0107** (ang. *accuracy*) czyli ilo\u015b\u0107 procent poprawnie sklasyfikowanych przyk\u0142ad\u00f3w\n",
      "\n",
      "Przetestujmy jak sobie radzi nasz LogisticRegression na zbiorze Iris. Teraz we\u017amiemy wszystkie przyk\u0142ady do nauki"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Za\u0142adujmy jeszcze raz dane iris\n",
      "iris = load_iris()\n",
      "X,Y = iris.data, iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dzielimy (scikit-learn)\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Pierwszy przyk\u0142ad trenuj\u0105cy: \", X_train[0], \"Klasa: \", Y_train[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "drugi_model = SVC() \n",
      "drugi_model.fit(X_train, Y_train) # Wspominany interfejs! Po zrobieniu fit mamy lini\u0119 prost\u0105\n",
      "Y_test_predicted = drugi_model.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Przewidywania na pierwszych 10 przykladach: \", Y_test_predicted[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "print \"Dok\u0142adno\u015b\u0107 modelu to: \",100*sklearn.metrics.accuracy_score(Y_test, Y_test_predicted), \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### \u0106wiczenie 4: Prosz\u0119 zamieni\u0107 LogisticRegression na inny model liniowy, sklearn.svm.SVC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### b) Preprocessing danych\n",
      "\n",
      "Wiele modeli lepiej dzia\u0142a na przekszta\u0142conych danych. Przekszta\u0142cenia to np.:\n",
      "    \n",
      "    * Normalizacja\n",
      "    * Skalowanie\n",
      "    * Przesuni\u0119cie\n",
      "    * Obr\u00f3t\n",
      "    * Zmniejszenie ilo\u015bci wymiar\u00f3w\n",
      "    \n",
      "Skupimy si\u0119 na ostatnim typie: zmniejszenie ilo\u015bci wymiar\u00f3w.\n",
      "    \n",
      "<img src=\"figures/dim-red.jpg\"></img>\n",
      "<img width=\"500\" src=\"figures/Inversion.png\"></img>\n",
      "\n",
      "Zmniejszenie ilo\u015bci wymiar\u00f3w (ang. *dimensionality reduction*) to takie przekszta\u0142cenie,\n",
      "kt\u00f3re zmniejsza wymiar danych (ilo\u015b\u0107 cech) gubi\u0105c minimaln\u0105 ilo\u015b\u0107 informacji.\n",
      "Czasami dane mo\u017cna opisa\u0107 mniejsz\u0105 ilo\u015bci\u0105 cech (mo\u017ce lepszych!).\n",
      "\n",
      "** Jest to bardzo wa\u017cna technika ** ze wzgl\u0119du na du\u017c\u0105 z\u0142o\u017cono\u015b\u0107 algorytm\u00f3w machine learning (np. SVM z \u0107wiczenia 4 ma minimaln\u0105 z\u0142o\u017cono\u015b\u0107 O(n^2) )"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### c) Hiperparametry\n",
      "\n",
      "Ka\u017cdy model opisywany jest hiperparametrami. Przyk\u0142adowo w regresji liniowej mo\u017cemy poda\u0107 \"fit_intercept\" co oznacza czy linia ma przechodzi przez \u015brodek wsp\u00f3\u0142rz\u0119dnych czy nie. Innym parametrem jest C - intuicyjnie mo\u017cna to rozumie\u0107 tak, \u017ce model stara si\u0119 przypisa\u0107 ma\u0142e wagi cechom."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(LogisticRegression)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### \u0106wiczenie 5: Prosz\u0119 spr\u00f3bowa\u0107 zmieni\u0107 parametr C w LogisticRegression tak aby otrzyma\u0107 100% dok\u0142adno\u015bci"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}